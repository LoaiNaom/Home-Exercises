---
title: "Ex7"
author: "Loai Naom"
date: "22/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Classification

For this set of exercises we will be using the gene expression and patient annotation data from the glioblastoma patient.The glioblastoma tumor samples are from The Cancer Genome Atlas project. We will try to predict the subtype of this disease using molecular markers. This subtype is characterized by large-scale epigenetic alterations called the “CpG island methylator phenotype” or “CIMP”

```{r}
library(compGenomRData)
library(caret)
# get file paths
fileLGGexp=system.file("extdata",
                      "LGGrnaseq.rds",
                      package="compGenomRData")
fileLGGann=system.file("extdata",
                      "patient2LGGsubtypes.rds",
                      package="compGenomRData")
# gene expression values
gexp=readRDS(fileLGGexp)

# patient annotation
patient=readRDS(fileLGGann)

head(gexp[,1:5])
dim(gexp)

head(patient)
dim(patient)


```

1. Our first task is to not use any data transformation and do classification. Run the k-NN classifier on the data without any transformation or scaling. What is the effect on classification accuracy for k-NN predicting the CIMP and noCIMP status of the patient? 


Here we will preprocess the data before we start training. This might include exploratory data analysis to see how variables and samples relate to each other. For example, we might want to check the correlation between predictor variables and keep only one variable from that group. In addition, some training algorithms might be sensitive to data scales or outliers. We should deal with those issues in this step. In some cases, the data might have missing values. We can choose to remove the samples that have missing values or try to impute them. Many machine learning algorithms will not be able to deal with missing values.

```{r}

# With transformation and scaling

# The first thing we will do is data normalization and transformation. We have to take care of data scale issues that might come from how the experiments are performed and the potential problems that might occur during data collection. Ideally, each tumor sample has a similar distribution of gene expression values. Systematic differences between tumor samples must be corrected. We check if there are such differences using box plots. We will only plot the first 50 tumor samples so that the figure is not too squished

boxplot(gexp[,1:50],outline=FALSE,col="cornflowerblue")

#It seems there was some normalization done on this data. Gene expression values per sample seem to have the same scale. However, it looks like they have long-tailed distributions, so a log transformation may fix that. These long-tailed distributions have outliers and this might adversely affect the models. Below, we show the effect of log transformation on the gene expression profile of a patient. We add a pseudo count of 1 to avoid log(0)

par(mfrow=c(1,2))
hist(gexp[,5],xlab="gene expression",main="",border="blue4",
     col="cornflowerblue")
hist(log10(gexp+1)[,5], xlab="gene expression log scale",main="",
     border="blue4",col="cornflowerblue")

#Since taking a log seems to work to tame the extreme values, we do that to our data and also add 1 pseudo-count to be able to deal with 0 values:
gexp=log10(gexp+1)

# transpose the data set
tgexp <- t(gexp)

# filter predictor variables which have low variation
#The more variables, the slower the algorithms will be generally.

# Datasets come sometimes with predictors that take a unique value across samples. Such uninformative predictor is more common than you might think. This kind of predictor is not only non-informative, it can break some models you may want to fit to your data (see example below - remove near zero variation (nzv) for the columns at least
# 85% of the values are the same
# this function creates the filter but doesn't apply it yet
nzv=preProcess(tgexp,method="nzv",uniqueCut = 15)

# apply the filter using "predict" function
# return the filtered dataset and assign it to nzv_tgexp - without the near-zero variation
# variable
nzv_tgexp=predict(nzv,tgexp)

dim(tgexp)
dim(nzv_tgexp)

tgexp <- nzv_tgexp

# In addition, we can also choose arbitrary cutoffs for variability. For example, we can choose to take the top 1000 variable predictors.
SDs=apply(tgexp,2,sd )#variation of each gene
topPreds=order(SDs,decreasing = TRUE)[1:1000]
tgexp=tgexp[,topPreds]

# We can scale the data. When we scale, each value of the predictor variable is divided by its standard deviation. Therefore predictor variables will have the same standard deviation. These manipulations are generally used to improve the numerical stability of some calculations. In distance-based metrics, it could be beneficial to at least center the data. We will now center the data using the preProcess() function. This is more practical than the scale() function because when we get a new data point, we can use the predict() function and processCenter object to process it just like we did for the training samples.

processCenter=preProcess(tgexp, method = c("center"))
tgexp=predict(processCenter,tgexp)

# create a filter for removing highly correlated variables
# if two variables are highly correlated only one of them is removed
corrFilt=preProcess(tgexp, method = "corr",cutoff = 0.9)
tgexp=predict(corrFilt,tgexp)

# For demonstration purposes, we will now introduce NA values in our data, the “NA” value is normally used to encode missing values in R. We then show how to check and deal with those. One way is to impute them; here, we again use a machine learning algorithm to guess the missing values. Another option is to discard the samples with missing values or discard the predictor variables with missing values. First, we replace one of the values as NA and check if it is there.

missing_tgexp=tgexp
missing_tgexp[1,1]=NA
anyNA(missing_tgexp) # check if there are NA values

gexpnoNA=missing_tgexp[ , colSums(is.na(missing_tgexp)) == 0] # the gene which contains NA is gone


# We will next try to impute the missing value(s). Imputation can be as simple as assigning missing values to the mean or median value of the variable, or assigning the mean/median of values from nearest neighbors of the sample having the missing value. We will show both using the caret::preProcess() function. First, let us run the median imputation.

mImpute=preProcess(missing_tgexp,method="medianImpute")
imputedGexp=predict(mImpute,missing_tgexp)

# Splitting the data - into the test and the training partitions. The reason for this is that we need an independent test we did not train on. 

tgexp=merge(patient,tgexp,by="row.names")

# push sample ids back to the row names
rownames(tgexp)=tgexp[,1]
tgexp=tgexp[,-1]

# There are multiple data split strategies. For starters, we will split 30% of the data as the test. This method is the gold standard for testing performance of our model.

set.seed(3031)
intrain <- createDataPartition(y = tgexp[,1], p= 0.7)[[1]]

# separate test and training sets
training <- tgexp[intrain,]
testing <- tgexp[-intrain,]

knnFit=knn3(x=training[,-1], # training set
            y=training[,1], # training set class labels
            k=5)
# predictions on the test set
trainPred=predict(knnFit,testing[,-1])

# predictions on the training set
trainPred=predict(knnFit,training[,-1],type="class")

# compare the predicted labels to real labels
# get different performance metrics
confusionMatrix(data=training[,1],reference=trainPred)

# predictions on the test set, return class labels
testPred=predict(knnFit,testing[,-1],type="class")

# compare the predicted labels to real labels
# get different performance metrics
confusionMatrix(data=testing[,1],reference=testPred)

```


```{r}
# Without any transformation or scaling
# this solution is also without log10 + 1

fileLGGexp=system.file("extdata",
                      "LGGrnaseq.rds",
                      package="compGenomRData")
fileLGGann=system.file("extdata",
                      "patient2LGGsubtypes.rds",
                      package="compGenomRData")
# gene expression values
gexp=readRDS(fileLGGexp)

# patient annotation
patient=readRDS(fileLGGann)

tgexp <- t(gexp)

# choosing the top 1000 variable predictors.
SDs_2 = apply(tgexp,2,sd )#variation of each gene
topPreds2=order(SDs_2,decreasing = TRUE)[1:1000]
tgexp=tgexp[,topPreds2]

# Splitting the data - into the test and the training partitions
tgexp=merge(patient,tgexp,by="row.names")

# push sample ids back to the row names
rownames(tgexp)=tgexp[,1]
tgexp=tgexp[,-1]

set.seed(3031)
intrain_2 <- createDataPartition(y = tgexp[,1], p= 0.7)[[1]]

# separate test and training sets
training2 <- tgexp[intrain_2,]
testing2 <- tgexp[-intrain_2,]

knnFitX=knn3(x=training2[,-1], # training set
            y=training2[,1], # training set class labels
            k=5)

# predictions on the test set
testPred_2=predict(knnFitX,testing2[,-1],type="class")

# compare the predicted labels to real labels
confusionMatrix(data=testing2[,1],reference=testPred_2)

# When taking the euclidean distance between pairs of samples, the values of the feature dimensions that range between small numbers may become uninformative and the algorithm would essentially rely on the single dimension whose values are substantially larger, and this effects the predictions.

# without transformation and scaling the accuracy of the predictions dropped, there are more false positives and false negatives. 

```

### Regression

For this set of problems we will use the regression data set where we tried to predict the age of the sample from the methylation values. The data can be loaded as shown below:

```{r}
# Predicting age from DNA methylation ~ the solution of this question was based on the course book

# file path for CpG methylation and age
fileMethAge=system.file("extdata",
                      "CpGmeth2Age.rds",
                      package="compGenomRData")

# read methylation-age table
ameth=readRDS(fileMethAge)
```

2. Run random forest regression and plot the importance metrics.
```{r}
# The solution for this question is based on the book

# Random Forest Regression consists of supervised learning algorithm for predicting output target feature average by bootstrap aggregation or bagging of independently built decision trees which are used for lowering variance error of the decision tree. Here, the target variable is the age and the predictor variables are the DNA methylation status. 

# methylation status can be measured with quantitative assays and the value is between 0 and 1. If it is 0, the CpG is not methylated in any of the cells in the sample, and if it is 1, the CpG is methylated in all the cells of the sample.

# plot histogram of methylation values
hist(unlist(ameth[,-1]),border="white",
      col="cornflowerblue",main="",xlab="methylation values")

# There are  27579 predictor variables (columns). We can remove the ones that have low variation across samples.The CpGs that have low variation are not likely to have any association with age; they could simply be technical variation of the experiment. We will remove CpGs that have less than 0.1 standard deviation.
ameth=ameth[,c(TRUE,matrixStats::colSds(as.matrix(ameth[,-1]))>0.1)]
dim(ameth)

# plot the predicted vs. observed ages and see how good our predictions are
set.seed(3031)
par(mfrow=c(1,2))

# we are not going to do any cross-validating and rely on OOB error
trainctrl <- trainControl(method = "none")

# we will now train a random forest model
ameth.rf <- train(Age~., 
               data = ameth, 
               method = "ranger",
               trControl=trainctrl,
               # calculate importance
               importance="permutation", 
               tuneGrid = data.frame(mtry=50,
                                     min.node.size = 5,
                                     splitrule="variance"))

# plot Observed vs OOB predicted values from the model
plot(ameth$Age,ameth.rf$finalModel$predictions,
     pch=19,xlab="observed Age",
     ylab="OOB predicted Age")
mtext(paste("R-squared",
            format(ameth.rf$finalModel$r.squared,digits=2)))

# plot residuals
plot(ameth$Age,(ameth.rf$finalModel$predictions-ameth$Age),
     pch=18,ylab="residuals (predicted-observed)",
     xlab="observed Age",col="blue3")
abline(h=0,col="red4",lty=2)

# Importance of the variables 
plot(varImp(ameth.rf),top=10)

#The model can capture the general trend and it has acceptable OOB performance. It is not perfect as it makes errors on average close to 10 years when predicting the age, and the errors are more severe for older people 

```